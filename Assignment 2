import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt



print("Results for Problem 1a")

#load and prepare data
df_1a = pd.read_csv('/content/Housing.csv')
X_1a = df_1a[['area', 'bedrooms', 'bathrooms', 'stories', 'parking']]
y_1a = df_1a['price']

#scale the selected features
scaler_1a = StandardScaler()
X_scaled_1a = scaler_1a.fit_transform(X_1a)
X_scaled_1a = pd.DataFrame(X_scaled_1a, columns=X_1a.columns)

#split the scaled data into training and validation sets 
X_train_1a, X_val_1a, y_train_1a, y_val_1a = train_test_split(X_scaled_1a, y_1a, test_size=0.2, random_state=42)

#linear regression with gradient descent
def hypothesis(X, theta):
    """Calculates the predicted values."""
    return X.dot(theta)

def compute_cost(X, y, theta):
    """Computes the mean squared error cost function."""
    m = len(y)
    predictions = hypothesis(X, theta)
    cost = (1/(2*m)) * np.sum((predictions - y)**2)
    return cost

def gradient_descent(X, y, theta, learning_rate, num_iterations):
    """Performs gradient descent."""
    m = len(y)
    cost_history = []

    for i in range(num_iterations):
        predictions = hypothesis(X, theta)
        errors = predictions - y
        gradient = (1/m) * X.T.dot(errors)
        theta -= learning_rate * gradient
        cost_history.append(compute_cost(X, y, theta))

    return theta, cost_history

#add intercept term to training and validation data 
X_train_intercept_1a = np.c_[np.ones(X_train_1a.shape[0]), X_train_1a]
X_val_intercept_1a = np.c_[np.ones(X_val_1a.shape[0]), X_val_1a]

#train the model with different learning rates 
learning_rates = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]
num_iterations = 1000
train_cost_histories_1a = {}
val_cost_histories_1a = {}
best_learning_rate_1a = None
min_val_cost_1a = float('inf')

for learning_rate in learning_rates:
    theta_1a = np.zeros(X_train_intercept_1a.shape[1])
    theta_trained_1a, train_cost_history_1a = gradient_descent(X_train_intercept_1a, y_train_1a, theta_1a, learning_rate, num_iterations)
    train_cost_histories_1a[learning_rate] = train_cost_history_1a

    val_cost_1a = compute_cost(X_val_intercept_1a, y_val_1a, theta_trained_1a)
    val_cost_histories_1a[learning_rate] = [val_cost_1a] * num_iterations

    if val_cost_1a < min_val_cost_1a:
        min_val_cost_1a = val_cost_1a
        best_learning_rate_1a = learning_rate

print(f"Best Learning Rate (1a): {best_learning_rate_1a}")
print(f"Minimum Validation Cost (1a): {min_val_cost_1a}")

#re-run gradient descent for the best learning rate to get the final theta 
theta_best_1a = np.zeros(X_train_intercept_1a.shape[1])
theta_best_1a, _ = gradient_descent(X_train_intercept_1a, y_train_1a, theta_best_1a, best_learning_rate_1a, num_iterations)

print("Best theta values (1a):")
print(theta_best_1a)

#plot losses for the best learning rate
plt.figure(figsize=(10, 6))
plt.plot(train_cost_histories_1a[best_learning_rate_1a], label='Training Loss (1a)')
plt.plot(val_cost_histories_1a[best_learning_rate_1a], label='Validation Loss (1a)')
plt.title(f'Training and Validation Loss over Iterations (1a) (Learning Rate: {best_learning_rate_1a})')
plt.xlabel('Iterations')
plt.ylabel('Cost')
plt.legend()
plt.show()



print("Results for Problem 1b")

#load and prepare data 
df_1b = pd.read_csv('/content/Housing.csv')
X_1b = df_1b[['area', 'bedrooms', 'bathrooms', 'stories', 'mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'parking', 'prefarea']]
y_1b = df_1b['price']

#handle categorical variables 
categorical_cols = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']
X_1b = pd.get_dummies(X_1b, columns=categorical_cols, drop_first=True)


#scale the selected features 
scaler_1b = StandardScaler()
X_scaled_1b = scaler_1b.fit_transform(X_1b)
X_scaled_1b = pd.DataFrame(X_scaled_1b, columns=X_1b.columns)

#split the scaled data into training and validation sets
X_train_1b, X_val_1b, y_train_1b, y_val_1b = train_test_split(X_scaled_1b, y_1b, test_size=0.2, random_state=42)

#add intercept term to training and validation data 
X_train_intercept_1b = np.c_[np.ones(X_train_1b.shape[0]), X_train_1b]
X_val_intercept_1b = np.c_[np.ones(X_val_1b.shape[0]), X_val_1b]

#train the model with different learning rates 
train_cost_histories_1b = {}
val_cost_histories_1b = {}
best_learning_rate_1b = None
min_val_cost_1b = float('inf')

for learning_rate in learning_rates:
    theta_1b = np.zeros(X_train_intercept_1b.shape[1])
    theta_trained_1b, train_cost_history_1b = gradient_descent(X_train_intercept_1b, y_train_1b, theta_1b, learning_rate, num_iterations)
    train_cost_histories_1b[learning_rate] = train_cost_history_1b

    val_cost_1b = compute_cost(X_val_intercept_1b, y_val_1b, theta_trained_1b)
    val_cost_histories_1b[learning_rate] = [val_cost_1b] * num_iterations

    if val_cost_1b < min_val_cost_1b:
        min_val_cost_1b = val_cost_1b
        best_learning_rate_1b = learning_rate

print(f"Best Learning Rate (1b): {best_learning_rate_1b}")
print(f"Minimum Validation Cost (1b): {min_val_cost_1b}")

#re-run gradient descent for the best final theta
theta_best_1b = np.zeros(X_train_intercept_1b.shape[1])
theta_best_1b, _ = gradient_descent(X_train_intercept_1b, y_train_1b, theta_best_1b, best_learning_rate_1b, num_iterations)

print("Best theta values (1b):")
print(theta_best_1b)

#plot losses for the best learning rate
plt.figure(figsize=(10, 6))
plt.plot(train_cost_histories_1b[best_learning_rate_1b], label='Training Loss (1b)')
plt.plot(val_cost_histories_1b[best_learning_rate_1b], label='Validation Loss (1b)')
plt.title(f'Training and Validation Loss over Iterations (1b) (Learning Rate: {best_learning_rate_1b})')
plt.xlabel('Iterations')
plt.ylabel('Cost')
plt.legend()
plt.show()

#comparison 

print("\nComparison of Problem 1a and 1b")
print(f"Minimum Validation Cost (1a): {min_val_cost_1a}")
print(f"Minimum Validation Cost (1b): {min_val_cost_1b}")

if min_val_cost_1b < min_val_cost_1a:
    print("Problem 1b performed lower validation cost")
else:
    print("Problem 1a performed lower validation cost or performance is similar")
